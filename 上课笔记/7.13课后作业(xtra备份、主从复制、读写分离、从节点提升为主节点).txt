7.13课后作业(xtra备份、主从复制、读写分离、从节点提升为主节点)


1、通过xtrabackupex备份数据库并恢复('全量备份')
	'备份的时候是在线的，还原的时候是离线的'

	(1) 安装xtrabackupex
		$ ntpdate 172.16.0.1
		$ wget ftp://172.16.0.1/pub/Sources/7.x86_64/percona/percona-xtrabackup-24-2.4.7-2.el7.x86_64.rpm
		$ yum install ./percona-xtrabackup-24-2.4.7-2.el7.x86_64.rpm

	(2) 备份数据库--全库
		$ vim /etc/my.cnf.d/server.conf
		[server]
		skip_name_resolve=ON
		innodb_file_per_table=ON
		log-bin=mysql_bin

		$ systemctl start mariadb.service

		$ innobackupex --user root /data/backup/
		$ ll /data/backup/2017-07-13_20-05-42/

	(3) 还原数据库
		1.准备操作：
		$ cp /var/lib/mysql/mysql_bin.00000* /data/backup
		$ cd /data/backup
		# 执行准备操作
		$ innobackupex --apply-log /data/backup/2017-07-13_20-05-42/

		2.删除数据库，模拟
		$ rm -rf /var/lib/mysql/*

		3.恢复
		$ innobackupex --copy-back 2017-07-13_20-05-42/
		$ cd /var/lib/mysql
		$ chown -R mysql.mysql ./*

		4.启动
		$ systemctl start mariadb.service
		$ mysql 
		> SHOW DATABASES;
















2、'增量备份：'
	 (1) 先做一次全量备份
	 	$ innobackupex --user root /data/backup

	 (2) 连接到数据库，删除/增加一些数据，之后做增量备份
	 	$ mysql
	 	> use hellodb;
	 	> DELETE FROM students WHERE StuID=21;
	 	> INSERT INTO students (Name,Age,Gender,ClassID,TeacherID) VALUES('Zhu Ba Jie',120,'M',2,3);
	 	> exit;

	 (3) 增量备份(第一次增量)
	 	$ innobackupex --incremental -u root /data/backup --incremental-basedir=/data/backup/2017-07-13_20-38-20/

	 (4) 再次连接到数据库，删除一些语句之后，再做第二次增量备份
	 	$ mysql 
	 	> use hellodb;
	 	> DELETE FROM students WHERE StuID=1;
	 	> EXIT;

	 (5) 增量备份(第二次)
	 	# 这里--incremental-basedir 为上一次增量备份的目录，而非全量备份的目录
	 	$ innobackupex -u root --incremental /data/backup --incremental-basedir=/data/backup/2017-07-13_20-47-49
	 	# 第二次增量的checkpoints
	 	$ cat 2017-07-13_20-56-20/xtrabackup_checkpoints
		backup_type = incremental
		from_lsn = 1638999
		to_lsn = 1640148
		last_lsn = 1640148
		compact = 0
		recover_binlog_info = 0
		
		# 第一次增量的checkpoints
		$ cat 2017-07-13_20-47-49/xtrabackup_checkpoints
		backup_type = incremental
		from_lsn = 1636913
		to_lsn = 1638999
		last_lsn = 1638999
		compact = 0
		recover_binlog_info = 0

	(6) 通过两次增量+全量恢复数据库
		$ rm -rf /var/lib/mysql/*

		1.合并增量备份(只提交不回滚)
			# 只提交不回滚 全量备份
			$ innobackupex --apply-log --redo-only 2017-07-13_20-38-20/

			# 只提交不回滚 第一次增量+全量
			$ innobackupex --apply-log --redo-only 2017-07-13_20-38-20 --incremental-dir=2017-07-13_20-47-49

			# 只提交不回滚 第二增量+全量(第一次增量+全量)
			$ innobackupex --apply-log --redo-only 2017-07-13_20-38-20 --incremental-dir=2017-07-13_20-56-20

		2.开始提交并回滚
			$ innobackupex --apply-log 2017-07-13_20-38-20/

		3.拿着提交并回滚后的全量，进行恢复
			$ innobackupex --copy-back 2017-07-13_20-38-20/

		4.查看恢复后的数据目录
			$ ll /var/lib/mysql 
			total 40988
			drwxr-x--- 2 root root     4096 Jul 13 21:17 2017-07-13_20-27-08
			drwxr-x--- 2 root root     4096 Jul 13 21:17 hellodb
			-rw-r----- 1 root root 18874368 Jul 13 21:17 ibdata1
			-rw-r----- 1 root root  5242880 Jul 13 21:17 ib_logfile0
			-rw-r----- 1 root root  5242880 Jul 13 21:17 ib_logfile1
			-rw-r----- 1 root root 12582912 Jul 13 21:17 ibtmp1
			drwxr-x--- 2 root root     4096 Jul 13 21:17 mysql
			drwxr-x--- 2 root root     4096 Jul 13 21:17 performance_schema
			drwxr-x--- 2 root root     4096 Jul 13 21:17 test
			-rw-r----- 1 root root       23 Jul 13 21:17 xtrabackup_binlog_pos_innodb
			-rw-r----- 1 root root      532 Jul 13 21:17 xtrabackup_info

		5.修改目录权限
			$ chown -R mysql.mysql /var/lib/mysql/*






















3、'主从复制'

	(1) 编辑'主节点配置文件'
		$ vim /etc/my.cnf.d/server.cnf
		[server]
		skip_name_resolve=ON
		innodb_file_per_table=ON
		# 开启二进制日志
		log-bin=mysql_bin

		# 设置服务器ID
		server_id=1

		$ systemctl restart mariadb.service 

	(2) 编辑'从节点配置文件'
		$ vim /etc/my.cnf.d/server.cnf
		[server]
		skip_name_resolve=ON
		innodb_file_per_table=ON
		# 开启二进制日志
		log-bin=mysql_bin

		# 设置服务器ID
		server_id=10

		# 启动中继日志
		relay_log=relay-log

		# 设置只读，对普通用户有效
		read_only=ON

		$ systemctl restart mariadb.service

	(3) 创建复制时使用的数据库用户
		$ mysql 
		> GRANT REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'copyuser'@'172.16.1.%' IDENTIFIED BY 'root@123';
		> FLUSH PRIVILEGES;
		> SHOW MASTER STATUS;
		+------------------+----------+--------------+------------------+
		| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB |
		+------------------+----------+--------------+------------------+
		| mysql_bin.000001 |      497 |              |                  |
		+------------------+----------+--------------+------------------+
		1 row in set (0.00 sec)

		这里的Position的497是之后在SLAVE节点时使用MASTER_LOG_POS= 这里填写的位置，也就是MASTER_LOG_POS=497

	(4) 从节点设置主节点的相关信息
		$ mysql 
		> CHANGE MASTER TO MASTER_USER='copyuser',MASTER_PASSWORD='root@123',MASTER_HOST='172.16.1.100',MASTER_PORT=3306,MASTER_LOG_FILE='mysql_bin.000001',MASTER_LOG_POS=497;
		> SHOW SLAVE STATUS\G
		*************************** 1. row ***************************
		               Slave_IO_State:
		               	  # 连接的主机IP地址
		                  Master_Host: 172.16.1.100
		                  # 连接主节点的用户
		                  Master_User: copyuser
		                  # 主节点的端口
		                  Master_Port: 3306
		                # 连接重试时间 60s
		                Connect_Retry: 60
		              # 主节点的二进制日志文件
		              Master_Log_File: mysql_bin.000001
		          # 从主节点二进制日志哪个位置开始读
		          Read_Master_Log_Pos: 497
		          	   # 中继日志
		               Relay_Log_File: relay-log.000001
		                # 在本机中继日志哪个位置开始记录
		                Relay_Log_Pos: 4
		        Relay_Master_Log_File: mysql_bin.000001
		             # IO Thread 是否运行
		             Slave_IO_Running: No
		            # SQL Thread 是否运行
		            Slave_SQL_Running: No
		              Replicate_Do_DB:
		          Replicate_Ignore_DB:
		           Replicate_Do_Table:
		       Replicate_Ignore_Table:
		      Replicate_Wild_Do_Table:
		  Replicate_Wild_Ignore_Table:
		                   Last_Errno: 0
		                   Last_Error:
		                 Skip_Counter: 0
		          # 从主节点复制到从节点的二进制日志位置，如果与Read_Master_Log_Pos的值相同，则从节点没有落后于主节点
		          Exec_Master_Log_Pos: 497
		              Relay_Log_Space: 245
		              Until_Condition: None
		               Until_Log_File:
		                Until_Log_Pos: 0
		           Master_SSL_Allowed: No
		           Master_SSL_CA_File:
		           Master_SSL_CA_Path:
		              Master_SSL_Cert:
		            Master_SSL_Cipher:
		               Master_SSL_Key:
		        # 落后于主节点多少秒
		        Seconds_Behind_Master: NULL
		Master_SSL_Verify_Server_Cert: No
						# 前一次IO线程错误码
		                Last_IO_Errno: 0
		                # 错误信息
		                Last_IO_Error:
		               # 前一次SQL线程错误码
		               Last_SQL_Errno: 0
		               # 错误信息
		               Last_SQL_Error:
		  # 忽略哪个SERVER ID
		  Replicate_Ignore_Server_Ids:
		  			 # 主节点Server_ID号
		             Master_Server_Id: 0
		1 row in set (0.00 sec)

	(5) 启动复制线程
		> START SLAVE;
		Query OK, 0 rows affected (0.00 sec)

		查看错误日志中对于复制线程记录的信息：
		$ cat /var/log/mysql/
		170713 21:55:20 [Note] 'CHANGE MASTER TO executed'. Previous state master_host='', master_port='3306', master_log_file='', master_log_pos='4'. New state master_host='172.16.1.100', master_port='3306', master_log_file='mysql_bin.000001', master_log_pos='497'.
		170713 22:05:25 [Note] Slave SQL thread initialized, starting replication in log 'mysql_bin.000001' at position 497, relay log './relay-log.000001' position: 4
		170713 22:05:25 [Note] Slave I/O thread: connected to master 'copyuser@172.16.1.100:3306',replication started in log 'mysql_bin.000001' at position 497
		

	(6) 查看启动复制线程后的SLAVE STATUS
		> SHOW SLAVE STATUS\G
		*************************** 1. row ***************************
					   # 等待主节点发送事件
		               Slave_IO_State: Waiting for master to send event
		                  Master_Host: 172.16.1.100
		                  Master_User: copyuser
		                  Master_Port: 3306
		                Connect_Retry: 60
		              Master_Log_File: mysql_bin.000001
		          Read_Master_Log_Pos: 497
		               Relay_Log_File: relay-log.000002
		                Relay_Log_Pos: 529
		        Relay_Master_Log_File: mysql_bin.000001
		             Slave_IO_Running: Yes
		            Slave_SQL_Running: Yes
		              Replicate_Do_DB:
		          Replicate_Ignore_DB:
		           Replicate_Do_Table:
		       Replicate_Ignore_Table:
		      Replicate_Wild_Do_Table:
		  Replicate_Wild_Ignore_Table:
		                   Last_Errno: 0
		                   Last_Error:
		                 Skip_Counter: 0
		          Exec_Master_Log_Pos: 497
		              Relay_Log_Space: 817
		              Until_Condition: None
		               Until_Log_File:
		                Until_Log_Pos: 0
		           Master_SSL_Allowed: No
		           Master_SSL_CA_File:
		           Master_SSL_CA_Path:
		              Master_SSL_Cert:
		            Master_SSL_Cipher:
		               Master_SSL_Key:
		        Seconds_Behind_Master: 0
		Master_SSL_Verify_Server_Cert: No
		                Last_IO_Errno: 0
		                Last_IO_Error:
		               Last_SQL_Errno: 0
		               Last_SQL_Error:
		  Replicate_Ignore_Server_Ids:
		             Master_Server_Id: 1
		1 row in set (0.00 sec)


	(7) 如果从节点出错，如何解决？
		1.停止复制线程
			> STOP SLAVE;
		2.重新设置MASTER_LOG_POS
			> CHANGE MASTER TO MASTER_USER='copyuser',MASTER_PASSWORD='root@123',MASTER_HOST='172.16.1.100',MASTER_PORT=3306,MASTER_LOG_FILE='mysql_bin.000002',MASTER_LOG_POS=245;
		3.启动复制线程
			> START SLAVE;

















4、'主主复制'
	(1) 编辑server_id=1的节点的配置文件
		$ vim /etc/my.cnf.d/server.cnf
		[server]
		skip_name_resolve=ON
		innodb_file_per_table=ON
		log-bin=mysql_bin

		server_id=1
		relay_log=relay-log
		# 起始偏移值
		auto_increment_offset=1
		# 步进值
		auto_increment_increment=2

	(2) 编辑server_id=10的节点的配置文件
		$ vim /etc/my.cnf.d/server.cnf
		[server]
		skip_name_resolve=ON
		innodb_file_per_table=ON
		log-bin=mysql_bin

		server_id=10
		relay_log=relay-log
		# 起始偏移值
		auto_increment_offset=2
		# 步进值
		auto_increment_increment=2


	(3) 两个主节点创建复制时使用的用户(两台节点都要执行下面的步骤)
		$ systemctl start mariadb.service 
		$ mysql 
		> GRANT REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'copyuser'@'172.16.1.%' IDENTIFIED BY 'root@123';
		> FLUSH PRIVILEGES;

	(4) 两台节点相互配置 主节点信息：
		> SHOW MASTER STATUS;

		server_id=1的节点：
		> CHANGE MASTER TO MASTER_USER='copyuser',MASTER_PASSWORD='root@123',MASTER_HOST='172.16.1.70',MASTER_PORT=3306,MASTER_LOG_FILE='mysql_bin.000003',MASTER_LOG_POS=507;

		server_id=10的节点：
		> CHANGE MASTER TO MASTER_USER='copyuser',MASTER_PASSWORD='root@123',MASTER_HOST='172.16.1.100',MASTER_PORT=3306,MASTER_LOG_FILE='mysql_bin.000003',MASTER_LOG_POS=507;

		查看是否正常
		> SHOW SLAVE STATUS\G

	(5) 开启从节点复制线程(两台都执行)
		> START SLAVE;
		> SHOW SLAVE STATUS\G

	(6) 测试：
		1.id=1的节点创建数据库mydb,并创建表tb1
			> CREATE DATABASE mydb;
			Query OK, 1 row affected (0.00 sec)
			> CREATE TABLE tb1 (id INT UNSIGNED NOT NULL PRIMARY KEY AUTO_INCREMENT,Name CHAR(30));
			Query OK, 0 rows affected (0.02 sec)

		2.id=10的节点use mydb,查看表结构
			> use mydb;
			Reading table information for completion of table and column names
			You can turn off this feature to get a quicker startup with -A

			Database changed

			> DESC tb1;
			+-------+------------------+------+-----+---------+----------------+
			| Field | Type             | Null | Key | Default | Extra          |
			+-------+------------------+------+-----+---------+----------------+
			| id    | int(10) unsigned | NO   | PRI | NULL    | auto_increment |
			| Name  | char(30)         | YES  |     | NULL    |                |
			+-------+------------------+------+-----+---------+----------------+

		3.id=1的节点插入tb1中数据
			> INSERT INTO tb1 (Name) VALUES ('stu1'),('stu2');
			Query OK, 2 rows affected (0.01 sec)
			Records: 2  Duplicates: 0  Warnings: 0

			> SELECT * FROM tb1;
			+----+------+
			| id | Name |
			+----+------+
			|  1 | stu1 |
			|  3 | stu2 |
			+----+------+
			2 rows in set (0.00 sec)

		4.id=10节点插入数据
			> INSERT INTO tb1 (Name) VALUES ('stu3'),('stu4');
			Query OK, 2 rows affected (0.02 sec)
			Records: 2  Duplicates: 0  Warnings: 0

			> SELECT * FROM tb1;
			+----+------+
			| id | Name |
			+----+------+
			|  1 | stu1 |
			|  3 | stu2 |
			|  4 | stu3 |
			|  6 | stu4 |
			+----+------+
			4 rows in set (0.00 sec)


		'由此实现主主复制'

	(6) 查看主主模式下的/var/lib/mysql下生成的文件：
		$ cat /var/lib/mysql/master.info  --> 此文件也是记录在内存缓冲区中，需要在MySQL配置文件中添加 'sync_master_info=ON'
		18
		# 从主节点的哪个二进制日志复制
		mysql_bin.000003
		# 复制到二进制日志的哪个位置
		838
		# 主节点IP
		172.16.1.100
		# 连接的用户
		copyuser
		# 密码
		root@123
		# 端口
		3306
		# 超时时间
		60
		0

		$ cat /var/lib/mysql/relay-log.info --> 配置文件中开启 -->  sync_relay_log_info=ON
		./relay-log.000002
		860
		mysql_bin.000003
		838





		0
		1800.000

		0














5、'半同步复制'

	(1) 配置两台数据库为主从复制
		主节点：
			$ vim /etc/my.cnf.d/server.cnf
			[server]
			skip_name_resolve=ON
			innodb_file_per_table=ON
			log-bin=mysql_bin

			server_id=1

		从节点：
			$ vim /etc/my.cnf.d/server.cnf
			[server]
			skip_name_resolve=ON
			innodb_file_per_table=ON
			log-bin=mysql_bin

			server_id=10
			relay_log=relay-log


	(2) 主节点创建复制时使用的用户
		> GRANT REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'copyuser'@'172.16.1.%' IDENTIFIED BY 'root@123';
		> FLUSH PRIVILEGES;
		> SHOW MASTER STATUS;
		+------------------+----------+--------------+------------------+
		| File             | Position | Binlog_Do_DB | Binlog_Ignore_DB |
		+------------------+----------+--------------+------------------+
		| mysql_bin.000003 |      497 |              |                  |
		+------------------+----------+--------------+------------------+
		1 row in set (0.00 sec)

	
	(3) 从节点配置主节点配置信息
		> CHANGE MASTER TO MASTER_USER='copyuser',MASTER_PASSWORD='root@123',MASTER_HOST='172.16.1.100',MASTER_PORT=3306,MASTER_LOG_FILE='mysql_bin.000003',MASTER_LOG_POS=497;
		Query OK, 0 rows affected (0.01 sec)

		MariaDB [(none)]> START SLAVE;
		Query OK, 0 rows affected (0.00 sec)

		MariaDB [(none)]> SHOW SLAVE STATUS\G


	(4) 安装半同步的插件
		'主节点：'
			> INSTALL PLUGIN rpl_semi_sync_master SONAME 'semisync_master.so';
			> SHOW PLUGINS;
			| rpl_semi_sync_master           | ACTIVE   | REPLICATION        | semisync_master.so | GPL     |

		'从节点：'
			> INSTALL PLUGIN rpl_semi_sync_slave SONAME 'semisync_slave.so';
			> SHOW PLUGINS;
			| rpl_semi_sync_slave            | ACTIVE   | REPLICATION        | semisync_slave.so | GPL     |


	(5) 启用插件
		查看是否启用插件：
			> SHOW GLOBAL VARIABLES LIKE '%semi%';
			+------------------------------------+-------+
			| Variable_name                      | Value |
			+------------------------------------+-------+
			| rpl_semi_sync_master_enabled       | OFF   | 			# 关闭状态，未开启
			| rpl_semi_sync_master_timeout       | 10000 | 			# 等待从节点的超时时间10000毫秒 --> 10s
			| rpl_semi_sync_master_trace_level   | 32    | 			# 内部信息的追踪级别
			| rpl_semi_sync_master_wait_no_slave | ON    | 			# 如果没有半同步节点，是否等待。默认为等待，不过超过超时时间，则不等待
			+------------------------------------+-------+
			4 rows in set (0.00 sec)

		'主节点'
			> SET @@global.rpl_semi_sync_master_enabled=ON;

		'从节点'
			> SET @@global.rpl_semi_sync_slave_enabled=ON;


	(6) SHOW GLOBAL STATUS LIKE 'rpl%';  --> 统计数据
		> SHOW GLOBAL STATUS  LIKE "rpl%";  
		+--------------------------------------------+-------------+
		| Variable_name                              | Value       |
		+--------------------------------------------+-------------+
		| Rpl_semi_sync_master_clients               | 0           | 		# 作为同步复制的从节点数量
		| Rpl_semi_sync_master_net_avg_wait_time     | 0           | 		# 平均等待时间 
		| Rpl_semi_sync_master_net_wait_time         | 0           | 		# 网络等待时间
		| Rpl_semi_sync_master_net_waits             | 0           |		# 等待了多少次
		| Rpl_semi_sync_master_no_times              | 0           | 		
		| Rpl_semi_sync_master_no_tx                 | 0           |		
		| Rpl_semi_sync_master_status                | ON          |		
		| Rpl_semi_sync_master_timefunc_failures     | 0           |
		| Rpl_semi_sync_master_tx_avg_wait_time      | 0           | 		# 事务等待平均时间
		| Rpl_semi_sync_master_tx_wait_time          | 0           |		# 事务等待时间
		| Rpl_semi_sync_master_tx_waits              | 0           |		# 事务等待次数
		| Rpl_semi_sync_master_wait_pos_backtraverse | 0           |
		| Rpl_semi_sync_master_wait_sessions         | 0           |
		| Rpl_semi_sync_master_yes_tx                | 0           |
		| Rpl_status                                 | AUTH_MASTER |
		+--------------------------------------------+-------------+
		15 rows in set (0.00 sec)

	(7) 从节点开启半同步复制
		> STOP SLAVE IO_THREAD;
		> START SLAVE IO_THREAD;

	(8) 主节点测试半同步：
		$ mysql < hellodb.sql 
		$ mysql 
		> SHOW GLOBAL STATUS  LIKE "rpl%";
		+--------------------------------------------+-------------+
		| Variable_name                              | Value       |
		+--------------------------------------------+-------------+
		| Rpl_semi_sync_master_clients               | 1           |
		| Rpl_semi_sync_master_net_avg_wait_time     | 167         |
		| Rpl_semi_sync_master_net_wait_time         | 5849        |
		| Rpl_semi_sync_master_net_waits             | 35          |
		| Rpl_semi_sync_master_no_times              | 0           |
		| Rpl_semi_sync_master_no_tx                 | 0           |
		| Rpl_semi_sync_master_status                | ON          |
		| Rpl_semi_sync_master_timefunc_failures     | 0           |
		| Rpl_semi_sync_master_tx_avg_wait_time      | 170         |
		| Rpl_semi_sync_master_tx_wait_time          | 5634        |
		| Rpl_semi_sync_master_tx_waits              | 33          |
		| Rpl_semi_sync_master_wait_pos_backtraverse | 0           |
		| Rpl_semi_sync_master_wait_sessions         | 0           |
		| Rpl_semi_sync_master_yes_tx                | 35          |
		| Rpl_status                                 | AUTH_MASTER |
		+--------------------------------------------+-------------+
		15 rows in set (0.00 sec)


	(9) '设置从节点只复制指定数据库'
		> SHOW GLOBAL VARIABLES LIKE '%do_db%';
		+-----------------+-------+
		| Variable_name   | Value |
		+-----------------+-------+
		| replicate_do_db |       |
		+-----------------+-------+
		1 row in set (0.00 sec)

		> STOP SLAVE;
		> SET @@global.replicate_do_db=mydb;
		> START SLAVE;
		> SHOW GLOBAL VARIABLES LIKE '%do_db%';
		+-----------------+-------+
		| Variable_name   | Value |
		+-----------------+-------+
		| replicate_do_db | mydb  |
		+-----------------+-------+
		1 row in set (0.00 sec)

		> SHOW SLAVE STATUS\G
		*************************** 1. row ***************************
		               Slave_IO_State: Waiting for master to send event
		                  Master_Host: 172.16.1.100
		                  Master_User: copyuser
		                  Master_Port: 3306
		                Connect_Retry: 60
		              Master_Log_File: mysql_bin.000003
		          Read_Master_Log_Pos: 7907
		               Relay_Log_File: relay-log.000004
		                Relay_Log_Pos: 529
		        Relay_Master_Log_File: mysql_bin.000003
		             Slave_IO_Running: Yes
		            Slave_SQL_Running: Yes
		              Replicate_Do_DB: mydb


		1.测试复制：
			'主节点'
			> DROP TABLE hellodb.teachers;

			'从节点'
			> use hellodb;
			Reading table information for completion of table and column names
			You can turn off this feature to get a quicker startup with -A

			Database changed
			
			> SHOW TABLES;
			+-------------------+
			| Tables_in_hellodb |
			+-------------------+
			| classes           |
			| coc               |
			| courses           |
			| scores            |
			| students          |
			| toc               |
			+-------------------+
			6 rows in set (0.00 sec)


		2.测试mydb是否可以正常复制：
			'主节点'：
				> CREATE DATABASE mydb;
				> use mydb;
				> CREATE TABLE tb1 (id INT(10),Name CHAR(20));

			'从节点'：
				> use mydb;
				> SHOW TABLES;
				+----------------+
				| Tables_in_mydb |
				+----------------+
				| tb1            |
				+----------------+
				1 row in set (0.00 sec)









6、清理日志PURGE
	(1) 备份mysql_bin二进制日志

	(2) 开始清除日志
		> FLUSH LOGS;
		> PURGE MASTER LOGS TO 'mysql_bin.000004';

	(3) 查看mysql_bin.index
		$ cat /var/lib/mysql/mysql_bin.index
		./mysql_bin.000004









7、'主从复制的读写分离'
	'同步时间'

	(1) 在上一个实验的基础上，也就是一个主从的基础上，再做一台从节点。
		不过之前的从节点配置了只同步一个数据库，我们需要将其配置修改一下：
			> STOP SLAVE;
			> SET @@global.replicate_do_db='';
			> START SLAVE;

		
		'主节点'：备份全库
			$ mysqldump -uroot --all-databases -R -E --triggers -x --master-data=2 > /root/alldb.sql 
			$ scp alldb.sql 172.16.1.21:/root

		
		'另一台从节点' IP: 172.16.1.21
			$ less alldb.sql 
			...
			-- CHANGE MASTER TO MASTER_LOG_FILE='mysql_bin.000004', MASTER_LOG_POS=245;
			...

			$ vim /etc/my.cnf.d/server.cnf
			[server]
			skip_name_resolve=ON
			innodb_file_per_table=ON

			server_id=20
			relay_log=relay-log
			read-only=ON

			$ systemctl start mariadb.service
			$ mysql < alldb.sql 
			$ mysql 
			> CHANGE MASTER TO MASTER_HOST='172.16.1.100',MASTER_USER='copyuser',MASTER_PASSWORD='root@123',MASTER_LOG_FILE='mysql_bin.000004',MASTER_LOG_POS=245,MASTER_PORT=3306;
			Query OK, 0 rows affected (0.03 sec)
			> SHOW SLAVE STATUS\G
			> START SLAVE;

		1.'主节点创建一个数据库，测试一下两个从节点是否可以正常复制：'
			> CREATE DATABASE testdb;
			> use testdb;
			> CREATE TABLE tb1 (id INT(10));
			> CREATE DATABASE testdb;
			Query OK, 1 row affected (10.00 sec)

			MariaDB [(none)]> SHOW DATABASES;
			+--------------------+
			| Database           |
			+--------------------+
			| information_schema |
			| hellodb            |
			| my_db              |
			| mydb               |
			| mysql              |
			| performance_schema |
			| test               |
			| testdb             |
			+--------------------+
			8 rows in set (0.00 sec)

		2.'从节点查看：'
			SLAVE1：
				> SHOW DATABASES;
				+--------------------+
				| Database           |
				+--------------------+
				| information_schema |
				| hellodb            |
				| my_db              |
				| mydb               |
				| mysql              |
				| performance_schema |
				| test               |
				| testdb             |
				+--------------------+
				8 rows in set (0.00 sec)

			SLAVE2：
				> SHOW DATABASES;
				+--------------------+
				| Database           |
				+--------------------+
				| information_schema |
				| hellodb            |
				| my_db              |
				| mydb               |
				| mysql              |
				| performance_schema |
				| test               |
				| testdb             |
				+--------------------+
				8 rows in set (0.00 sec)


	(2) '在主节点创建一个给proxySQL进行远程登录MySQL数据库的账号(从节点自动同步)'

		> GRANT ALL ON *.* TO 'myadmin'@'172.16.1.%' IDENTIFIED BY 'root@123';
		> SELECT user,host,password FROM mysql.user;
		+---------+------------+-------------------------------------------+
		| user    | host       | password                                  |
		+---------+------------+-------------------------------------------+
		| root    | localhost  |                                           |
		| root    | test-2     |                                           |
		| root    | 127.0.0.1  |                                           |
		| root    | ::1        |                                           |
		|         | localhost  |                                           |
		|         | test-2     |                                           |
		| myadmin | 172.16.1.% | *A00C34073A26B40AB4307650BFB9309D6BFA6999 |
		+---------+------------+-------------------------------------------+
		7 rows in set (0.00 sec)




	(3) '在新的一台虚拟机上安装proxySQL：'
		$ wget ftp://172.16.0.1/pub/Sources/7.x86_64/proxysql/proxysql-1.3.6-1-centos7.x86_64.rpm
		$ yum install ./proxysql-1.3.6-1-centos7.x86_64.rpm
		# 备份配置文件
		$ cp /etc/proxysql.cnf{,.bak}

		# 自定义proxysql的配置文件
		$ vim /etc/proxysql.cnf
		datadir="/var/lib/proxysql"

		# 登陆proxysql管理接口，进行管理时使用的配置
		admin_variables=
		{
				# 用户名和密码
		        admin_credentials="admin:admin"
		        # 监听的端口或者本地连接sock
		        mysql_ifaces="127.0.0.1:6032;/tmp/proxysql_admin.sock"
		#       refresh_interval=2000
		#       debug=true
		}


		mysql_variables=
		{
				# 并发线程，单线程响应多个请求。建议为CPU核心数	
		        threads=4
		        # 最大连接数
		        max_connections=2048
		        # 默认查询延迟时间
		        default_query_delay=0
		        # 默认查询超时时间
		        default_query_timeout=36000000
		        # 是否压缩
		        have_compress=true
		        # 轮询时超时时间
		        poll_timeout=2000
		        # MySQL监听端口
		        interfaces="0.0.0.0:3306;/tmp/proxysql.sock"
		        # 用户连接后默认使用的数据库
		        default_schema="my_db"
		        # 栈大小
		        stacksize=1048576
		        # 服务器版本
		        server_version="5.5.30"
		        # 连接超时时间
		        connect_timeout_server=3000
		        monitor_history=600000
		        monitor_connect_interval=60000
		        monitor_ping_interval=10000
		        monitor_read_only_interval=1500
		        monitor_read_only_timeout=500
		        ping_interval_server=120000
		        ping_timeout_server=500
		        commands_stats=true
		        sessions_sort=true
		        connect_retries_on_failure=10
		}

		
		# defines all the MySQL servers
		# MySQL服务器组
		mysql_servers =
		(
		       {
		       		   # 数据库服务器IP地址
		       		   # 主节点
		               address = "172.16.1.100" # no default, required . If port is 0 , address is interpred as a Unix Socket Domain
		               port = 3306           # no default, required . If port is 0 , address is interpred as a Unix Socket Domain
		               # 主机组
		               hostgroup = 0           # no default, required
		               # 在线还是离线
		               status = "ONLINE"     # default: ONLINE
		               # 权重
		               weight = 1            # default: 1
		               # 是否压缩
		               compression = 0       # default: 0
		               # 最大并发连接数
		               max_connections=200
		   # 如果是读服务器，是否启用延迟。尽量不要开启
		   # max_replication_lag = 10  # default 0 . If greater than 0 and replication lag passes such threshold, the server is shunned
		       }
		       {
		       			# 从节点1
		                address = "172.16.1.70" # no default, required . If port is 0 , address is interpred as a Unix Socket Domain
		                port = 3306           # no default, required . If port is 0 , address is interpred as a Unix Socket Domain
		                # 从节点主机组为 1
		                hostgroup = 1           # no default, required
		                status = "ONLINE"     # default: ONLINE
		                weight = 1            # default: 1
		                compression = 0       # default: 0
		                max_connections=500
		        },
		        {
		        		# 从节点2
		                address = "172.16.1.21" # no default, required . If port is 0 , address is interpred as a Unix Socket Domain
		                port = 3306           # no default, required . If port is 0 , address is interpred as a Unix Socket Domain
		                # 从节点主机组为 1
		                hostgroup = 1           # no default, required
		                status = "ONLINE"     # default: ONLINE
		                weight = 1            # default: 1
		                compression = 0       # default: 0
		                max_connections=500
		        }
		        # 注意最后一个括号，这个后面没有逗号


		# 登录后端MySQL主机的账号和密码
		mysql_users:
		(
		       {
		               # 用户名
		               username = "myadmin" # no default , required
		               # 密码
		               password = "root@123" # default: ''
		               # 默认连接到哪个组中，默认连接到主服务组
		               default_hostgroup = 0 # default: 0
		               # 是否激活
		               active = 1            # default: 1
		               # 连接后默认使用的数据库
		               default_schema="my_db"
		       }
		#       {
		#               username = "root"
		#               password = ""
		#               default_hostgroup = 0
		#               max_connections=1000
		#               default_schema="test"
		#               active = 1
		#       },
		#       { username = "user1" , password = "password" , default_hostgroup = 0 , active = 0 }
		)



		#defines MySQL Query Rules
		# 语句路由，将指定的SQL语句，路由到指定的服务器上
		mysql_query_rules:
		(
		#       {
		#               rule_id=1
		#               active=1
		#               match_pattern="^SELECT .* FOR UPDATE$"
		#               destination_hostgroup=0
		#               apply=1
		#       },
		#       {
		#               rule_id=2
		#               active=1
		#               match_pattern="^SELECT"
		#               destination_hostgroup=1
		#               apply=1
		#       }
		)


		# MySQL复制集群的定义：指定读组、写组
		# 这里组号是之前在mysql_servers段中设置的hostgroup的ID
		mysql_replication_hostgroups=
		(
		        {
		        		# 写组
		                writer_hostgroup=30
		                # 读组
		                reader_hostgroup=40
		                # 注释信息
		                comment="test repl 1"
		        }
		#       {
		#                writer_hostgroup=50
		#                reader_hostgroup=60
		#                comment="test repl 2"
		#        }
		)


	(4) 启动ProxySQL并连接数据库
		
		$ systemctl start proxysql 
		# 这里的用户和密码是MySQL的密码，而非proxySQL的管理账号密码
		$ mysql -h 172.16.1.40 -umyadmin -proot@123
		Welcome to the MariaDB monitor.  Commands end with ; or \g.
		Your MySQL connection id is 3
		Server version: 5.5.30 (ProxySQL)

		Copyright (c) 2000, 2015, Oracle, MariaDB Corporation Ab and others.

		Type 'help;' or '\h' for help. Type '\c' to clear the current input statement.

		MySQL [(none)]> SHOW DATABASES;
		+--------------------+
		| Database           |
		+--------------------+
		| information_schema |
		| hellodb            |
		| my_db              |
		| mydb               |
		| mysql              |
		| performance_schema |
		| test               |
		+--------------------+
		7 rows in set (0.00 sec)

	(5) 通过ProxySQL连接的数据库进行创建表，进行测试(默认连接的是主服务器，具有读写功能)
		> use my_db;
		> CREATE TABLE tb1 (id int(10));
		> SHOW TABLES;
		+-----------------+
		| Tables_in_my_db |
		+-----------------+
		| tb1             |
		+-----------------+
		1 row in set (0.00 sec)

		在主服务器和两台从节点查看是否创建了tb1这张表，如果有，则成功

	(6) ProxySQL节点查看主从节点信息：
		$ mysql -uadmin -padmin -h127.0.0.1 -P6032
		> select hostgroup_id,hostname,port,status from mysql_servers;
		+--------------+--------------+------+--------+
		| hostgroup_id | hostname     | port | status |
		+--------------+--------------+------+--------+
		| 0            | 172.16.1.100 | 3306 | ONLINE |
		| 1            | 172.16.1.70  | 3306 | ONLINE |
		| 1            | 172.16.1.21  | 3306 | ONLINE |
		+--------------+--------------+------+--------+
		3 rows in set (0.00 sec)

		# 查看backend MySQL
		> SELECT * FROM mysql_servers;
		+--------------+--------------+------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+
		| hostgroup_id | hostname     | port | status | weight | compression | max_connections | max_replication_lag | use_ssl | max_latency_ms | comment |
		+--------------+--------------+------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+
		| 0            | 172.16.1.100 | 3306 | ONLINE | 1      | 0           | 200             | 0                   | 0       | 0              |         |
		| 1            | 172.16.1.70  | 3306 | ONLINE | 1      | 0           | 500             | 0                   | 0       | 0              |         |
		| 1            | 172.16.1.21  | 3306 | ONLINE | 1      | 0           | 500             | 0                   | 0       | 0              |         |
		+--------------+--------------+------+--------+--------+-------------+-----------------+---------------------+---------+----------------+---------+
		3 rows in set (0.00 sec)

		# 查看组信息
		> SELECT * FROM runtime_mysql_replication_hostgroups;
		+------------------+------------------+-----------+
		| writer_hostgroup | reader_hostgroup | comment   |
		+------------------+------------------+-----------+
		| 0                | 1                | my repl 1 |
		+------------------+------------------+-----------+
		1 row in set (0.00 sec)





















8、'MHA MySQL HA 配置'
	
	(1) 在上一个实验的基础上执行如下操作：修改主从节点的配置文件
		'主节点：'
			$ vim /etc/my.cnf.d/server.cnf
			[server]
			skip_name_resolve=ON
			innodb_file_per_table=ON
			log-bin=mysql_bin

			server_id=1
			relay_log=relay-log

		'从节点：1'
			[server]
			skip_name_resolve=ON
			innodb_file_per_table=ON
			log-bin=mysql_bin

			server_id=10
			relay_log=relay-log
			# 关闭中继日志清除功能
			relay_log_purge=0
			# 开启只读
			read-only=1

		'从节点：2'
			[server]
			skip_name_resolve=ON
			innodb_file_per_table=ON
			log_bin=mysql-bin

			server_id=20
			relay_log=relay-log
			relay_log_purge=0
			read-only=1

		重启服务：
		$ systemctl restart mariadb.serivce 


	(2) 生成密钥对，并复制给其他节点
		'主节点'
			$ ssh-keygen -t rsa -P ''
			$ ssh-copy-id -i .ssh/id_rsa.pub root@172.16.1.100
			$ scp -p .ssh/authorized_keys .ssh/id_rsa{,.pub} .ssh/known_hosts root@172.16.1.70:/root/.ssh/
			$ scp -p .ssh/authorized_keys .ssh/id_rsa{,.pub} .ssh/known_hosts root@172.16.1.21:/root/.ssh/
			$ scp -p .ssh/authorized_keys .ssh/id_rsa{,.pub} .ssh/known_hosts root@172.16.1.40:/root/.ssh/


	(3) 下载MHA manager
		官方下载：https://github.com/yoshinorim/mha4mysql-manager

		'管理节点(这里使用之前ProxySQL的主机作为管理节点)'
		$ wget ftp://172.16.0.1/pub/Sources/6.x86_64/mha/mha4mysql-manager-0.56-0.el6.noarch.rpm
		$ wget ftp://172.16.0.1/pub/Sources/6.x86_64/mha/mha4mysql-node-0.56-0.el6.noarch.rpm


		'主从节点下载node包'
		$ wget ftp://172.16.0.1/pub/Sources/6.x86_64/mha/mha4mysql-node-0.56-0.el6.noarch.rpm

	
	(4) 安装MHA

		'管理节点：' 两个包都需要安装 manager、node
		$ yum install -y ./mha4mysql-manager-0.56-0.el6.noarch.rpm
		$ yum install -y ./mha4mysql-node-0.56-0.el6.noarch.rpm

		'主从节点'
		$ yum install -y ./mha4mysql-node-0.56-0.el6.noarch.rpm

	
	(5) 主节点创建MHA远程连接管理的用户
		> GRANT ALL PRIVILEGES ON *.* TO 'mhaadmin'@'172.16.1.%' IDENTIFIED BY 'mhapass';
		> FLUSH PRIVILEGES;

	
	(6) 配置管理节点
		$ vim /etc/masterha/app1.cnf
		# 通用配置
		[server default] 
		# 管理员账号
		user=mhaadmin # MySQL Administrator 
		# 管理员密码
		password=mhapass # MySQL Administrator's password 
		# 管理工作路径，会自动创建
		manager_workdir=/data/masterha/app1 
		# 管理日志路径
		manager_log=/data/masterha/app1/manager.log 
		# 远程节点的工作路径(node节点)
		remote_workdir=/data/masterha/app1 
		# ssh用户账号
		ssh_user=root 
		# 复制时用的账号
		repl_user=copyuser 
		# 复制时用的密码
		repl_password=root@123
		# ping探测间隔，1s
		ping_interval=1


		# 各server节点的专用配置，server1、2、3为固定格式
		[server1] 
		# 主机地址配置，可以是主机名，也可以是IP地址。
		hostname=172.16.1.100
		#ssh_port=22022 
		# master候选主机，当主节点宕机，这些主机可以成为master
		candidate_master=1

		[server2] 
		hostname=172.16.1.70
		#ssh_port=22022 
		candidate_master=1

		[server3] 
		hostname=172.16.1.21
		#ssh_port=22022 
		#no_master=1


	(7) 检查通信：使用 masterha_check_ssh命令，指定配置文件路径

		$ masterha_check_ssh --conf=/etc/masterha/app1.cnf
		Fri Jul 14 11:13:47 2017 - [warning] Global configuration file /etc/masterha_default.cnf not found. Skipping.
		Fri Jul 14 11:13:47 2017 - [info] Reading application default configuration from /etc/masterha/app1.cnf..
		Fri Jul 14 11:13:47 2017 - [info] Reading server configuration from /etc/masterha/app1.cnf..
		Fri Jul 14 11:13:47 2017 - [info] Starting SSH connection tests..
		Fri Jul 14 11:13:47 2017 - [debug]
		Fri Jul 14 11:13:47 2017 - [debug]  Connecting via SSH from root@172.16.1.100(172.16.1.100:22) to root@172.16.1.70(172.16.1.70:22)..
		Fri Jul 14 11:13:47 2017 - [debug]   ok.
		Fri Jul 14 11:13:47 2017 - [debug]  Connecting via SSH from root@172.16.1.100(172.16.1.100:22) to root@172.16.1.21(172.16.1.21:22)..
		Fri Jul 14 11:13:47 2017 - [debug]   ok.
		Fri Jul 14 11:13:48 2017 - [debug]
		Fri Jul 14 11:13:47 2017 - [debug]  Connecting via SSH from root@172.16.1.70(172.16.1.70:22) to root@172.16.1.100(172.16.1.100:22)..
		Fri Jul 14 11:13:47 2017 - [debug]   ok.
		Fri Jul 14 11:13:47 2017 - [debug]  Connecting via SSH from root@172.16.1.70(172.16.1.70:22) to root@172.16.1.21(172.16.1.21:22)..
		Fri Jul 14 11:13:47 2017 - [debug]   ok.
		Fri Jul 14 11:13:48 2017 - [debug]
		Fri Jul 14 11:13:48 2017 - [debug]  Connecting via SSH from root@172.16.1.21(172.16.1.21:22) to root@172.16.1.100(172.16.1.100:22)..
		Fri Jul 14 11:13:48 2017 - [debug]   ok.
		Fri Jul 14 11:13:48 2017 - [debug]  Connecting via SSH from root@172.16.1.21(172.16.1.21:22) to root@172.16.1.70(172.16.1.70:22)..
		Fri Jul 14 11:13:48 2017 - [debug]   ok.
		Fri Jul 14 11:13:48 2017 - [info] All SSH connection tests passed successfully.

		$ masterha_check_repl --conf=/etc/masterha/app1.cnf
		Fri Jul 14 11:15:16 2017 - [warning] Global configuration file /etc/masterha_default.cnf not found. Skipping.
		Fri Jul 14 11:15:16 2017 - [info] Reading application default configuration from /etc/masterha/app1.cnf..
		Fri Jul 14 11:15:16 2017 - [info] Reading server configuration from /etc/masterha/app1.cnf..
		Fri Jul 14 11:15:16 2017 - [info] MHA::MasterMonitor version 0.56.
		Creating directory /data/masterha/app1.. done.
		Fri Jul 14 11:15:16 2017 - [info] GTID failover mode = 0
		Fri Jul 14 11:15:16 2017 - [info] Dead Servers:
		Fri Jul 14 11:15:16 2017 - [info] Alive Servers:
		Fri Jul 14 11:15:16 2017 - [info]   172.16.1.100(172.16.1.100:3306)
		Fri Jul 14 11:15:16 2017 - [info]   172.16.1.70(172.16.1.70:3306)
		Fri Jul 14 11:15:16 2017 - [info]   172.16.1.21(172.16.1.21:3306)
		Fri Jul 14 11:15:16 2017 - [info] Alive Slaves:
		Fri Jul 14 11:15:16 2017 - [info]   172.16.1.70(172.16.1.70:3306)  Version=5.5.44-MariaDB-log (oldest major version between slaves) log-bin:enabled
		Fri Jul 14 11:15:16 2017 - [info]     Replicating from 172.16.1.100(172.16.1.100:3306)
		Fri Jul 14 11:15:16 2017 - [info]     Primary candidate for the new Master (candidate_master is set)
		Fri Jul 14 11:15:16 2017 - [info]   172.16.1.21(172.16.1.21:3306)  Version=5.5.44-MariaDB-log (oldest major version between slaves) log-bin:enabled
		Fri Jul 14 11:15:16 2017 - [info]     Replicating from 172.16.1.100(172.16.1.100:3306)
		Fri Jul 14 11:15:16 2017 - [info] Current Alive Master: 172.16.1.100(172.16.1.100:3306)
		Fri Jul 14 11:15:16 2017 - [info] Checking slave configurations..
		Fri Jul 14 11:15:16 2017 - [info] Checking replication filtering settings..
		Fri Jul 14 11:15:16 2017 - [info]  binlog_do_db= , binlog_ignore_db=
		Fri Jul 14 11:15:16 2017 - [info]  Replication filtering check ok.
		Fri Jul 14 11:15:16 2017 - [error][/usr/share/perl5/vendor_perl/MHA/Server.pm, ln393] 172.16.1.70(172.16.1.70:3306): User copyuser does not exist or does not have REPLICATION SLAVE privilege! Other slaves can not start replication from this host.
		Fri Jul 14 11:15:16 2017 - [error][/usr/share/perl5/vendor_perl/MHA/MasterMonitor.pm, ln424] Error happened on checking configurations.  at /usr/share/perl5/vendor_perl/MHA/ServerManager.pm line 1403.
		Fri Jul 14 11:15:16 2017 - [error][/usr/share/perl5/vendor_perl/MHA/MasterMonitor.pm, ln523] Error happened on monitoring servers.
		Fri Jul 14 11:15:16 2017 - [info] Got exit code 1 (Not master dead).

		MySQL Replication Health is NOT OK!

		这里错误是因为，从节点没有copyuser这个用户。
		这时，我们再建一个用户，用于复制即可：
			'主节点'
			> GRANT REPLICATION CLIENT, REPLICATION SLAVE ON *.* TO 'copy'@'172.16.1.%' IDENTIFIED BY 'root@123';
			> FLUSH PRIVILEGES;

		'修改配置文件：'
		$ vim /etc/masterha/app1.cnf
		repl_user=copy


		'重新检查：'
		$ masterha_check_repl --conf=/etc/masterha/app1.cnf
		Fri Jul 14 11:20:38 2017 - [warning] Global configuration file /etc/masterha_default.cnf not found. Skipping.
		Fri Jul 14 11:20:38 2017 - [info] Reading application default configuration from /etc/masterha/app1.cnf..
		Fri Jul 14 11:20:38 2017 - [info] Reading server configuration from /etc/masterha/app1.cnf..
		Fri Jul 14 11:20:38 2017 - [info] MHA::MasterMonitor version 0.56.
		Fri Jul 14 11:20:39 2017 - [info] GTID failover mode = 0
		Fri Jul 14 11:20:39 2017 - [info] Dead Servers:
		Fri Jul 14 11:20:39 2017 - [info] Alive Servers:
		Fri Jul 14 11:20:39 2017 - [info]   172.16.1.100(172.16.1.100:3306)
		Fri Jul 14 11:20:39 2017 - [info]   172.16.1.70(172.16.1.70:3306)
		Fri Jul 14 11:20:39 2017 - [info]   172.16.1.21(172.16.1.21:3306)
		Fri Jul 14 11:20:39 2017 - [info] Alive Slaves:
		Fri Jul 14 11:20:39 2017 - [info]   172.16.1.70(172.16.1.70:3306)  Version=5.5.44-MariaDB-log (oldest major version between slaves) log-bin:enabled
		Fri Jul 14 11:20:39 2017 - [info]     Replicating from 172.16.1.100(172.16.1.100:3306)
		Fri Jul 14 11:20:39 2017 - [info]     Primary candidate for the new Master (candidate_master is set)
		Fri Jul 14 11:20:39 2017 - [info]   172.16.1.21(172.16.1.21:3306)  Version=5.5.44-MariaDB-log (oldest major version between slaves) log-bin:enabled
		Fri Jul 14 11:20:39 2017 - [info]     Replicating from 172.16.1.100(172.16.1.100:3306)
		Fri Jul 14 11:20:39 2017 - [info] Current Alive Master: 172.16.1.100(172.16.1.100:3306)
		Fri Jul 14 11:20:39 2017 - [info] Checking slave configurations..
		Fri Jul 14 11:20:39 2017 - [info] Checking replication filtering settings..
		Fri Jul 14 11:20:39 2017 - [info]  binlog_do_db= , binlog_ignore_db=
		Fri Jul 14 11:20:39 2017 - [info]  Replication filtering check ok.
		Fri Jul 14 11:20:41 2017 - [info] Checking replication health on 172.16.1.70..
		Fri Jul 14 11:20:41 2017 - [info]  ok.
		Fri Jul 14 11:20:41 2017 - [info] Checking replication health on 172.16.1.21..
		Fri Jul 14 11:20:41 2017 - [info]  ok.
		Fri Jul 14 11:20:41 2017 - [warning] master_ip_failover_script is not defined.
		Fri Jul 14 11:20:41 2017 - [warning] shutdown_script is not defined.
		Fri Jul 14 11:20:41 2017 - [info] Got exit code 0 (Not master dead).

		MySQL Replication Health is OK.



	(8) 检查通过后，开始启动MHA
		$ nohup masterha_manager --conf=/etc/masterha/app1.cnf &> /data/masterha/app1/manager.log  &
		$ ps aux 
		root      2453  4.6  2.1 298668 21504 pts/0    S    11:32   0:00 perl /usr/bin/masterha_manager --conf=/etc/masterha/app1.cnf

	(9) 检查master状态
		$ masterha_check_status --conf=/etc/masterha/app1.cnf
		app1 (pid:2453) is running(0:PING_OK), master:172.16.1.100

		上面的信息中"app1 (pid:4978) is running(0:PING_OK)"表示 MHA 服务运行 OK，否则，则会显示为类似"app1 is stopped(1:NOT_RUNNING)."。

	(10) 测试故障转移
		1.在master节点，关闭mariadb服务，测试故障转移是否成功
			$ killall -9 mysqld mysqld_safe
			$ ss -tnl | grep 3306

		2.MHA manager节点：
			$ ps aux 
			[1]+  Done                    nohup masterha_manager --conf=/etc/masterha/app1.cnf &>/data/masterha/app1/manager.log
			在完成故障切换之后，manager会自动停止

			$ masterha_check_status --conf=/etc/masterha/app1.cnf
			app1 is stopped(2:NOT_RUNNING).

		3.查看从节点是否升级为主节点
			$ less /data/masterha/app1/manager.log
			----- Failover Report -----

			app1: MySQL Master failover 172.16.1.100(172.16.1.100:3306) to 172.16.1.70(172.16.1.70:3306) succeeded

			Master 172.16.1.100(172.16.1.100:3306) is down!

			Check MHA Manager logs at localhost.localdomain:/data/masterha/app1/manager.log for details.

			Started automated(non-interactive) failover.
			The latest slave 172.16.1.70(172.16.1.70:3306) has all relay logs for recovery.
			Selected 172.16.1.70(172.16.1.70:3306) as a new master.
			172.16.1.70(172.16.1.70:3306): OK: Applying all logs succeeded.
			172.16.1.21(172.16.1.21:3306): This host has the latest relay log events.
			Generating relay diff files from the latest slave succeeded.
			172.16.1.21(172.16.1.21:3306): OK: Applying all logs succeeded. Slave started, replicating from 172.16.1.70(172.16.1.70:3306)
			172.16.1.70(172.16.1.70:3306): Resetting slave info succeeded.
			Master failover to 172.16.1.70(172.16.1.70:3306) completed successfully.

			'从节点：172.16.1.70' -> 已经提升为主节点
				> SHOW SLAVE HOSTS;
				+-----------+------+------+-----------+
				| Server_id | Host | Port | Master_id |
				+-----------+------+------+-----------+
				|        20 |      | 3306 |        10 |
				+-----------+------+------+-----------+
				1 row in set (0.00 sec)

				> CREATE DATABASE mhatest;
				Query OK, 1 row affected (0.00 sec)

				成功创建数据库

			'从节点：172.16.1.21'
				查看是否成功创建数据库
				> SHOW DATABASES;
				+--------------------+
				| Database           |
				+--------------------+
				| information_schema |
				| hellodb            |
				| mhatest            |
				| my_db              |
				| mydb               |
				| mysql              |
				| performance_schema |
				| test               |
				+--------------------+
				8 rows in set (0.00 sec)

			'现在已经成功实现故障转移'


	(12) 修复之前的master为slave
		现Master节点：1.70
			$ mysqldump -uroot -x -R -E --triggers --master-data=2 --all-databases > alldb.sql
			$ scp alldb.sql root@172.16.1.100:/root

		1.100：
			$ vim /etc/my.cnf.d/server.cnf
			[server]
			skip_name_resolve=ON
			innodb_file_per_table=ON
			log-bin=mysql_bin

			server_id=1
			relay_log=relay-log
			relay_log_purge=0
			read_only=1

			$ systemctl start mariadb 
			$ mysql < alldb.sql  
			$ mysql 
			> SHOW DATABASES;
			+--------------------+
			| Database           |
			+--------------------+
			| information_schema |
			| hellodb            |
			| mhatest            |
			| my_db              |
			| mydb               |
			| mysql              |
			| performance_schema |
			| test               |
			+--------------------+
			8 rows in set (0.00 sec)

			> CHANGE MASTER TO MASTER_HOST='172.16.1.70',MASTER_USER='copy',MASTER_PASSWORD='root@123',MASTER_LOG_FILE='mysql_bin.000005',MASTER_LOG_POS=334,MASTER_PORT=3306;
			> START SLAVE;
			# 查看是否正常运行
			> SHOW SLAVE STATUS\G
			# 刷新授权，否则manager无法连接
			> FLUSH PRIVILEGES;


		'主节点'
			> SHOW SLAVE HOSTS;
			+-----------+------+------+-----------+
			| Server_id | Host | Port | Master_id |
			+-----------+------+------+-----------+
			|         1 |      | 3306 |        10 |
			|        20 |      | 3306 |        10 |
			+-----------+------+------+-----------+
			2 rows in set (0.00 sec)

		

		'MHA节点检测：'
			$ masterha_check_repl --conf=/etc/masterha/app1.cnf
			Fri Jul 14 12:01:17 2017 - [info] Checking replication health on 172.16.1.100..
			Fri Jul 14 12:01:17 2017 - [info]  ok.
			Fri Jul 14 12:01:17 2017 - [info] Checking replication health on 172.16.1.21..
			Fri Jul 14 12:01:17 2017 - [info]  ok.
			Fri Jul 14 12:01:17 2017 - [warning] master_ip_failover_script is not defined.
			Fri Jul 14 12:01:17 2017 - [warning] shutdown_script is not defined.
			Fri Jul 14 12:01:17 2017 - [info] Got exit code 0 (Not master dead).

			MySQL Replication Health is OK.

			# 再次启动MHA manager，防止主节点宕机。开始监控
			$ nohup masterha_manager --conf=/etc/masterha/app1.cnf &> /data/masterha/app1/manager.log  &


		'恢复完成'


	(13) 停止MHA
		$ masterha_stop --conf=/etc/masterha/app1.cnf




								 


